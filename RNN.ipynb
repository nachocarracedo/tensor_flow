{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "In this chapter, we\n",
    "are going to discuss recurrent neural networks (RNN), a class of nets that can predict\n",
    "the future (well, up to a point, of course). They can analyze time series data such as\n",
    "stock prices, and tell you when to buy or sell. In autonomous driving systems, they\n",
    "can anticipate car trajectories and help avoid accidents. More generally, they can work\n",
    "on sequences of arbitrary lengths, rather than on fixed-sized inputs like all the nets we\n",
    "have discussed so far. For example, they can take sentences, documents, or audio\n",
    "samples as input, making them extremely useful for natural language processing\n",
    "(NLP) systems such as automatic translation, speech-to-text, or sentiment analysis\n",
    "(e.g., reading movie reviews and extracting the rater’s feeling about the movie).\n",
    "Moreover, RNNs’ ability to anticipate also makes them capable of surprising creativity.\n",
    "You can ask them to predict which are the most likely next notes in a melody, then\n",
    "randomly pick one of these notes and play it. Then ask the net for the next most likely\n",
    "notes, play it, and repeat the process again and again. Before you know it, your net\n",
    "will compose a melody such as the one produced by Google’s Magenta project. Similarly,\n",
    "RNNs can generate sentences, image captions, and much more. The result is not\n",
    "exactly Shakespeare or Mozart yet, but who knows what they will produce a few years\n",
    "from now?\n",
    "In this chapter, we will look at the fundamental concepts underlying RNNs, the main\n",
    "problem they face (namely, vanishing/exploding gradients, discussed in Chapter 11),\n",
    "and the solutions widely used to fight it: LSTM and GRU cells. Along the way, as\n",
    "always, we will show how to implement RNNs using TensorFlow. Finally, we will take\n",
    "a look at the architecture of a machine translation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

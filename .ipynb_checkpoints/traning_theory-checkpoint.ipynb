{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING THEORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VANISHING GRADIENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, gradients often get smaller and smaller as the algorithm progresses\n",
    "down to the lower layers. As a result, the Gradient Descent update leaves the lower\n",
    "layer connection weights virtually unchanged, and training never converges to a good\n",
    "solution. This is called the vanishing gradients problem. In some cases, the opposite\n",
    "can happen: the gradients can grow bigger and bigger, so many layers get insanely\n",
    "large weight updates and the algorithm diverges. This is the exploding gradients problem,\n",
    "which is mostly encountered in recurrent neural networks (see Chapter 14).\n",
    "More generally, deep neural networks suffer from unstable gradients; different layers\n",
    "may learn at widely different speeds.\n",
    "\n",
    "#### Xavier and He Initialization\n",
    "\n",
    "Using the Xavier initialization strategy can speed up training considerably, and it is\n",
    "one of the tricks that led to the current success of Deep Learning. Some recent papers4\n",
    "have provided similar strategies for different activation functions, as shown in\n",
    "Table 11-1. The initialization strategy for the ReLU activation function (and its variants,\n",
    "including the ELU activation described shortly) is sometimes called He initialization\n",
    "(after the last name of its author).\n",
    "\n",
    "#### Nonsaturating Activation Functions\n",
    "\n",
    "\n",
    "So which activation function should you use for the hidden layers\n",
    "of your deep neural networks? Although your mileage will vary, in\n",
    "general ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic.\n",
    "If you care a lot about runtime performance, then you may prefer\n",
    "leaky ReLUs over ELUs. If you don’t want to tweak yet another\n",
    "hyperparameter, you may just use the default α values suggested\n",
    "earlier (0.01 for the leaky ReLU, and 1 for ELU). If you have spare\n",
    "time and computing power, you can use cross-validation to evaluate\n",
    "other activation functions, in particular RReLU if your network\n",
    "is overfitting, or PReLU if you have a huge training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow offers an elu() function that you can use to build your neural network.\n",
    "Simply set the activation_fn argument when calling the fully_connected() function,\n",
    "like this:\n",
    "\n",
    "`hidden1 = fully_connected(X, n_hidden1, activation_fn=tf.nn.elu)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow does not have a predefined function for leaky ReLUs, but it is easy\n",
    "enough to define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "hidden1 = fully_connected(X, n_hidden1, activation_fn=leaky_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although using He initialization along with ELU (or any variant of ReLU) can significantly\n",
    "reduce the vanishing/exploding gradients problems at the beginning of training,\n",
    "it doesn’t guarantee that they won’t come back during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch normalization (good for very deep networks)\n",
    "\n",
    "The authors demonstrated that this technique considerably improved all the deep\n",
    "neural networks they experimented with. The vanishing gradients problem was\n",
    "strongly reduced, to the point that they could use saturating activation functions such\n",
    "as the tanh and even the logistic activation function. The networks were also much\n",
    "less sensitive to the weight initialization. They were able to use much larger learning\n",
    "rates, significantly speeding up the learning process. Specifically, they note that\n",
    "“Applied to a state-of-the-art image classification model, Batch Normalization achieves\n",
    "the same accuracy with 14 times fewer training steps, and beats the original\n",
    "model by a significant margin. […] Using an ensemble of batch-normalized networks,\n",
    "we improve upon the best published result on ImageNet classification: reaching\n",
    "4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of\n",
    "human raters.” Finally, like a gift that keeps on giving, Batch Normalization also acts\n",
    "like a regularizer, reducing the need for other regularization techniques (such as\n",
    "dropout, described later in the chapter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization does, however, add some complexity to the model (although it\n",
    "removes the need for normalizing the input data since the first hidden layer will take\n",
    "care of that, provided it is batch-normalized). Moreover, there is a runtime penalty:\n",
    "the neural network makes slower predictions due to the extra computations required\n",
    "at each layer. So if you need predictions to be lightning-fast, you may want to check\n",
    "how well plain ELU + He initialization perform before playing with Batch Normalization.\n",
    "\n",
    "Implementing Batch Normalization with TensorFlow\n",
    "\n",
    "\n",
    "TensorFlow provides a batch_normalization() function that simply centers and\n",
    "normalizes the inputs, but you must compute the mean and standard deviation yourself\n",
    "(based on the mini-batch data during training or on the full dataset during testing,\n",
    "as just discussed) and pass them as parameters to this function, and you must\n",
    "also handle the creation of the scaling and offset parameters (and pass them to this\n",
    "function). It is doable, but not the most convenient approach. Instead, you should use\n",
    "the batch_norm() function, which handles all this for you. You can either call it\n",
    "directly or tell the fully_connected() function to use it, such as in the following\n",
    "code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "# bool to tell if it's training data and we use moving average for mean and std or\n",
    "# is test and we use mean and std from traning\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "bn_params = {\n",
    "    'is_training': is_training,\n",
    "    'decay': 0.99,\n",
    "    'updates_collections': None\n",
    "    }\n",
    "\n",
    "hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\",\n",
    "normalizer_fn=batch_norm, normalizer_params=bn_params)\n",
    "hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\",\n",
    "normalizer_fn=batch_norm, normalizer_params=bn_params)\n",
    "logits = fully_connected(hidden2, n_outputs, activation_fn=None,scope=\"outputs\",\n",
    "normalizer_fn=batch_norm, normalizer_params=bn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid repeating the same parameters over\n",
    "and over again, you can create an argument scope using the arg_scope() function:\n",
    "the first parameter is a list of functions, and the other parameters will be passed to\n",
    "these functions automatically. The last three lines of the preceding code can be modified\n",
    "like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.contrib.framework.arg_scope(\n",
    "        [fully_connected],\n",
    "        normalizer_fn=batch_norm,\n",
    "        normalizer_params=bn_params):\n",
    "    hidden1 = fully_connected(X, n_hidden1, scope=\"hidden1\")\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, scope=\"hidden2\")\n",
    "    logits = fully_connected(hidden2, n_outputs, scope=\"outputs\", activation_fn=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the construction phase is the same as in Chapter 10: define the cost function,\n",
    "create an optimizer, tell it to minimize the cost function, define the evaluation\n",
    "operations, create a Saver, and so on.\n",
    "The execution phase is also pretty much the same, with one exception. Whenever you\n",
    "run an operation that depends on the batch_norm layer, you need to set the is_train\n",
    "ing placeholder to True or False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        [...]\n",
    "        for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={is_training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_score = accuracy.eval(feed_dict={is_training: False, X: X_test_scaled, y: y_test}))\n",
    "        print(accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "A popular technique to lessen the exploding gradients problem is to simply clip the\n",
    "gradients during backpropagation so that they never exceed some threshold (this is\n",
    "mostly useful for recurrent neural networks; see Chapter 14). This is called Gradient\n",
    "Clipping.8 In general people now prefer Batch Normalization, but it’s still useful to\n",
    "know about Gradient Clipping and how to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REUSING PRETRAINED LAYERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally not a good idea to train a very large DNN from scratch: instead, you\n",
    "should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle, then just reuse the lower layers of this network:\n",
    "this is called transfer learning. It will not only speed up training considerably, but will\n",
    "also require much less training data.\n",
    "\n",
    "If the input pictures of your new task don’t have the same size as\n",
    "the ones used in the original task, you will have to add a preprocessing\n",
    "step to resize them to the size expected by the original\n",
    "model. More generally, transfer learning will work only well if the\n",
    "inputs have similar low-level features.\n",
    "\n",
    "#### Reusing a TensorFlow Model\n",
    "If the original model was trained using TensorFlow, you can simply restore it and\n",
    "train it on the new task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[...] # construct the original model\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_original_model.ckpt\")\n",
    "    [...] # Train it on your new task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in general you will want to reuse only part of the original model (as we will\n",
    "discuss in a moment). A simple solution is to configure the Saver to restore only a\n",
    "subset of the variables from the original model. For example, the following code\n",
    "restores only hidden layers 1, 2, and 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[...] # build new model with the same definition as before for hidden layers 1-3\n",
    "init = tf.global_variables_initializer()\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[123]\")\n",
    "reuse_vars_dict = dict([(var.name, var.name) for var in reuse_vars])\n",
    "original_saver = tf.Saver(reuse_vars_dict) # saver to restore the original model\n",
    "new_saver = tf.Saver() # saver to save the new model\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    original_saver.restore(\"./my_original_model.ckpt\") # restore layers 1 to 3\n",
    "    [...] # train the new model\n",
    "    new_saver.save(\"./my_new_model.ckpt\") # save the whole model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we build the new model, making sure to copy the original model’s hidden layers\n",
    "1 to 3. We also create a node to initialize all variables. Then we get the list of all variables\n",
    "that were just created with \"trainable=True\" (which is the default), and we\n",
    "keep only the ones whose scope matches the regular expression \"hidden[123]\" (i.e.,\n",
    "we get all trainable variables in hidden layers 1 to 3). Next we create a dictionary\n",
    "mapping the name of each variable in the original model to its name in the new\n",
    "model (generally you want to keep the exact same names). Then we create a Saver\n",
    "that will restore only these variables, and we create another Saver to save the entire\n",
    "new model, not just layers 1 to 3. We then start a session and initialize all variables in\n",
    "the model, then restore the variable values from the original model’s layers 1 to 3.\n",
    "Finally, we train the model on the new task and save it.\n",
    "\n",
    "The more similar the tasks are, the more layers you want to reuse\n",
    "(starting with the lower layers). For very similar tasks, you can try\n",
    "keeping all the hidden layers and just replace the output layer.\n",
    "\n",
    "#### Freezing the Lower Layers\n",
    "It is likely that the lower layers of the first DNN have learned to detect low-level features\n",
    "in pictures that will be useful across both image classification tasks, so you can\n",
    "just reuse these layers as they are. It is generally a good idea to “freeze” their weights\n",
    "when training the new DNN: if the lower-layer weights are fixed, then the higherlayer\n",
    "weights will be easier to train (because they won’t have to learn a moving target).\n",
    "To freeze the lower layers during training, the simplest solution is to give the optimizer\n",
    "the list of variables to train, excluding the variables from the lower layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|outputs\")\n",
    "training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output layer. This leaves out the variables in the hidden layers 1 and 2. Next we provide\n",
    "this restricted list of trainable variables to the optimizer’s minimize() function.\n",
    "Ta-da! Layers 1 and 2 are now frozen: they will not budge during training (these are\n",
    "often called frozen layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching the Frozen Layers\n",
    "Since the frozen layers won’t change, it is possible to cache the output of the topmost\n",
    "frozen layer for each training instance. Since training goes through the whole dataset\n",
    "many times, this will give you a huge speed boost as you will only need to go through\n",
    "the frozen layers once per training instance (instead of once per epoch). For example,\n",
    "you could first run the whole training set through the lower layers (assuming you\n",
    "have enough RAM).\n",
    "\n",
    "Then during training, instead of building batches of training instances, you would\n",
    "build batches of outputs from hidden layer 2 and feed them to the training operation.\n",
    "\n",
    "The last line runs the training operation defined earlier (which freezes layers 1 and 2),\n",
    "and feeds it a batch of outputs from the second hidden layer (as well as the targets for\n",
    "that batch). Since we give TensorFlow the output of hidden layer 2, it does not try to\n",
    "evaluate it (or any node it depends on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden2_outputs = sess.run(hidden2, feed_dict={X: X_train})\n",
    "\n",
    "import numpy as np\n",
    "n_epochs = 100\n",
    "n_batches = 500\n",
    "for epoch in range(n_epochs):\n",
    "    shuffled_idx = rnd.permutation(len(hidden2_outputs))\n",
    "    hidden2_batches = np.array_split(hidden2_outputs[shuffled_idx], n_batches)\n",
    "    y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
    "    for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "        sess.run(training_op, feed_dict={hidden2: hidden2_batch, y: y_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then try unfreezing one or two of the top hidden layers to let backpropagation tweak\n",
    "them and see if performance improves. The more training data you have, the more\n",
    "layers you can unfreeze.\n",
    "If you still cannot get good performance, and you have little training data, try dropping\n",
    "the top hidden layer(s) and freeze all remaining hidden layers again. You can iterate until you find the right number of layers to reuse. If you have plenty of training\n",
    "data, you may try replacing the top hidden layers instead of dropping them, and\n",
    "even add more hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FASTER OPTIMIZERS\n",
    "\n",
    "the most popular ones: Momentum optimization,\n",
    "Nesterov Accelerated Gradient, AdaGrad, RMSProp, and finally Adam\n",
    "optimization.\n",
    "\n",
    "Adam usually yields good results but it has 3 parameters. Defaults are good but in case I want to tweak them you need to dig dipper in what they do and you need to understand the others in order to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REGULARIZATION (avoding overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
